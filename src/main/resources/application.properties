server.port=${SERVER_PORT:9091}

# Actuator exposure
management.endpoints.web.exposure.include=${MANAGEMENT_ENDPOINTS_WEB_EXPOSURE_INCLUDE:health,info}
management.endpoint.health.probes.enabled=${MANAGEMENT_ENDPOINT_HEALTH_PROBES_ENABLED:true}
management.tracing.enabled=${MANAGEMENT_TRACING_ENABLED:true}

# Raw payload dump configuration
webhook.rawDump.enabled=${WEBHOOK_RAW_DUMP_ENABLED:true}
webhook.rawDump.dir=${WEBHOOK_RAW_DUMP_DIR:webhook-dumps}

# Azure DevOps REST configuration
# ado.baseUrl=${ADO_BASE_URL}
# ado.projectId=${ADO_PROJECT_ID}
ado.pat=${ADO_PAT}

# PR changes dump
ado.changesDump.enabled=${ADO_CHANGES_DUMP_ENABLED:true}
ado.changesDump.dir=${ADO_CHANGES_DUMP_DIR:ado-pr-changes}

# File dumps for base/target contents
ado.filesDump.enabled=${ADO_FILES_DUMP_ENABLED:true}
ado.filesDump.dir=${ADO_FILES_DUMP_DIR:ado-pr-files}

# Spring AI Azure OpenAI
spring.ai.azure.openai.endpoint=${AZURE_OPENAI_ENDPOINT}
spring.ai.azure.openai.api-key=${AZURE_OPENAI_API_KEY}
spring.ai.azure.openai.chat.options.model=${AZURE_OPENAI_MODEL:gpt-4o-mini}
spring.ai.azure.openai.chat.options.temperature=${AZURE_OPENAI_TEMPERATURE:0.2}
spring.ai.azure.openai.api-version=${AZURE_OPENAI_API_VERSION:2025-01-01-preview}

# Async executor
spring.task.execution.pool.core-size=${SPRING_TASK_EXECUTION_POOL_CORE_SIZE:4}
spring.task.execution.pool.max-size=${SPRING_TASK_EXECUTION_POOL_MAX_SIZE:8}
spring.task.execution.pool.queue-capacity=${SPRING_TASK_EXECUTION_POOL_QUEUE_CAPACITY:100}

# LLM prompt/response logging
llm.promptLog.enabled=${LLM_PROMPT_LOG_ENABLED:true}
llm.promptLog.dir=${LLM_PROMPT_LOG_DIR:prompt-logs}
llm.responseLog.enabled=${LLM_RESPONSE_LOG_ENABLED:true}
llm.responseLog.dir=${LLM_RESPONSE_LOG_DIR:response-logs}

# File logging
logging.file.name=${LOGGING_FILE_NAME:logs/azure-pr-reviewer.log}
logging.level.root=${LOGGING_LEVEL_ROOT:INFO}
logging.level.TracingDebug=${LOGGING_LEVEL_TRACING_DEBUG:DEBUG}
logging.level.com.example.webhook.llm.PrReviewService=${LOGGING_LEVEL_COM_EXAMPLE_WEBHOOK_LLM_PRREVIEWSERVICE:DEBUG}

# Micrometer Tracing (OpenTelemetry) configuration
management.otlp.metrics.export.enabled=true
management.otlp.tracing.endpoint=${OTLP_ENDPOINT}
management.otlp.tracing.headers.Authorization=${OTLP_AUTH_HEADER}
management.tracing.sampling.probability=${MANAGEMENT_TRACING_SAMPLING_PROBABILITY:1.0}
management.tracing.propagation.type=${MANAGEMENT_TRACING_PROPAGATION_TYPE:b3}
management.observations.key-values.application=${MANAGEMENT_OBSERVATIONS_KEY_VALUES_APPLICATION:azure-pr-reviewer-webhook}

# Kafka / Confluent Cloud
kafka.bootstrapServers=${KAFKA_BOOTSTRAP_SERVERS}
kafka.security.protocol=${KAFKA_SECURITY_PROTOCOL}
kafka.sasl.mechanism=${KAFKA_SASL_MECHANISM}
kafka.sasl.jaas.config=${KAFKA_SASL_JAAS_CONFIG}
kafka.topic.azure.pr.events=${KAFKA_TOPIC_AZURE_PR_EVENTS}

# Kafka Producer Configuration for Large Messages
spring.kafka.producer.bootstrap-servers=${KAFKA_BOOTSTRAP_SERVERS}
spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializer
spring.kafka.producer.value-serializer=org.apache.kafka.common.serialization.StringSerializer
spring.kafka.producer.max-request-size=1048576000
spring.kafka.producer.batch-size=1048576
spring.kafka.producer.linger-ms=5
spring.kafka.producer.compression-type=gzip

# Kafka Consumer Configuration for Large Messages
spring.kafka.consumer.bootstrap-servers=${KAFKA_BOOTSTRAP_SERVERS}
spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer
spring.kafka.consumer.value-deserializer=org.apache.kafka.common.serialization.StringDeserializer
spring.kafka.consumer.max-partition-fetch-bytes=1048576000
spring.kafka.consumer.fetch-max-bytes=1048576000
spring.kafka.consumer.group-id=azure-pr-reviewer
spring.kafka.consumer.auto-offset-reset=earliest
spring.kafka.consumer.enable-auto-commit=true
spring.kafka.consumer.auto-commit-interval-ms=1000