server.port=9092
spring.application.name=reviewer-service

# Actuator exposure
management.endpoints.web.exposure.include=health,info
management.endpoint.health.probes.enabled=true

# Raw payload dump configuration
webhook.rawDump.enabled=false
webhook.rawDump.dir=webhook-dumps

# Azure DevOps Configuration
# ado.baseUrl=${ADO_BASE_URL}
# ado.pat=${ADO_PAT}

# PR changes dump
ado.changesDump.enabled=false
ado.changesDump.dir=ado-pr-changes

# File dumps for base/target contents
ado.filesDump.enabled=false
ado.filesDump.dir=ado-pr-files

# Azure OpenAI Configuration
spring.ai.azure.openai.endpoint=${AZURE_OPENAI_ENDPOINT}
spring.ai.azure.openai.api-key=${AZURE_OPENAI_API_KEY}
spring.ai.azure.openai.chat.options.model=${AZURE_OPENAI_MODEL}
spring.ai.azure.openai.chat.options.temperature=0.2
spring.ai.azure.openai.api-version=2025-01-01-preview

# Async executor
spring.task.execution.pool.core-size=4
spring.task.execution.pool.max-size=8
spring.task.execution.pool.queue-capacity=100

# LLM prompt/response logging
llm.promptLog.enabled=true
llm.promptLog.dir=prompt-logs
llm.responseLog.enabled=true
llm.responseLog.dir=response-logs

# File logging
logging.file.name=logs/reviewer-service.log
logging.level.root=INFO
logging.level.TracingDebug=DEBUG
logging.level.com.example.reviewer.llm.PrReviewService=DEBUG

# OTLP Tracing Configuration
management.otlp.metrics.export.enabled=true
management.otlp.tracing.endpoint=${OTLP_ENDPOINT}
management.otlp.tracing.headers.Authorization=${OTLP_AUTH_HEADER}
management.tracing.sampling.probability=1.0
management.tracing.propagation.type=b3
management.observations.key-values.application=reviewer-service

# Confluent consumer
kafka.bootstrapServers=${KAFKA_BOOTSTRAPSERVERS}
kafka.security.protocol=${KAFKA_SECURITY_PROTOCOL}
kafka.sasl.mechanism=${KAFKA_SASL_MECHANISM}
kafka.sasl.jaas.config=${KAFKA_SASL_JAAS_CONFIG}
kafka.topic.azure.pr.events=${KAFKA_TOPIC_AZURE_PR_EVENTS}



